
<!doctype html>
<html lang="en">
  <head>
    <script src="https://use.fontawesome.com/baff6f55f5.js"></script>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <title>Xiaojun Chen's Homepage</title>
  
      <link rel="stylesheet" href="stylesheets/styles.css">
      <link rel="stylesheet" href="stylesheets/github-light.css">
      <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
      <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
      <![endif]-->
  
      <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  
        ga('create', 'UA-29643011-3', 'auto');
        ga('send', 'pageview');
      </script>
  
      <!-- New GA4 tracking code, see https://support.google.com/analytics/answer/10271001#analyticsjs-enable-basic --> 
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-GNJD50R0Z7"></script>
      <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-GNJD50R0Z7');
      </script>
  
      <!-- For all browsers -->
      <link rel="stylesheet" href="assets/css/academicons.min.css"/>
      <link rel="stylesheet" href="assets/css/academicons.css"/>
  
      <style>
        button.accordion {
        font:14px/1.5 Lato, "Helvetica Neue", Helvetica, Arial, sans-serif;
        cursor: pointer;
        padding: 0px;
        border: none;
        text-align: left;
        outline: none;
        font-size: 100%;
        transition: 0.3s;
        background-color: #f8f8f8;
        }
        button.accordion.active, button.accordion:hover {
        background-color: #f8f8f8;
        }
        button.accordion:after {
        content: " [+] ";
        font-size: 90%;
        color:#777;
        float: left;
        margin-left: 1px;
        }
  
        button.accordion.active:after {
        content: " [\2212] ";
        }
        div.panel {
        padding: 0 20px;
        margin-top: 5px;
        display: none;
        background-color: white;
        font-size: 100%;
        }
        div.panel.show {
        display: block !important;
        }
        .social-row {
          display: flex;
          flex-wrap: wrap;
          justify-content: space-between;
        }
      </style>
    </head>
    <body>
      <div class="wrapper">
        <header>
          <h1>Xiaojun Chen</h1>
          <p>Associate Professor of Computer Science and Software Engineering<br><a href="https://www.szu.edu.cn/" target="_blank"></a>Shenzhen University</p>
          <p>Research Affiliate<br><a href="https://bigdata.szu.edu.cn/"  target="_blank">Big Data Institute</a></p>
      <h3><a href="https://bruce-xjchen.github.io/HomePage/">Home</a></h3>
          <h3><a href="https://bruce-xjchen.github.io/HomePage/research.html">Research</a></h3>
      <h3><a href="https://bruce-xjchen.github.io/HomePage/research/CV.pdf">CV</a></h3>  
          
          
          <h3><a href="https://bruce-xjchen.github.io/HomePage/workshops.html">Workshops</a></h3>
      <b>Social</b><br>
          <div class="social-row">
            <a href="mailto:xjchen@szu.edu.cn" class="author-social" target="_blank"><i class="fa fa-fw fa-envelope-square"></i> Email</a><br>
            <a href="https://scholar.google.com/citations?hl=en&user=yAjyrwkAAAAJ" target="_blank"><i class="ai ai-fw ai-google-scholar-square"></i> Scholar</a><br>
            <a href="https://orcid.org/0000-0002-2818-4652"><i class="ai ai-fw ai-orcid-square"></i> ORCID</a><br>
            <a href="https://github.com/Bruce-XJChen"><i class="fa fa-fw fa-github-square"></i> GitHub</a><br>
            <br>
          </div>
          <br>
  
          Zhiteng Building, Canghai Campus of Shenzhen University, Nanshan District, Shenzhen, Guangdong Province,
  China 518060
  
      <p><b>Contact:</b><br>College of Computer Science and Software Engineering<br>Shenzhen University<br>Room 614, Zhiteng Building, Canghai Campus of Shenzhen University<br>Shenzhen, Guangdong Province, China 518060</p>
      <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
        </header>



      <section>
        <h2><a id="recent-RRs-updated" class="anchor" href="#RRpapers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Papers under Revision to Resubmit</h2>
        <p style="margin:0"> None. <br> <br> 
    
        <hr>
    
        <h2><a id="recent-papers-updated" class="anchor" href="#workingpapers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Working Papers</h2>
        <p style="margin:0"> Coming soon. <br> <br> 
    
        <hr>
    
        <h2><a id="published-papers-updated" class="anchor" href="#publications" aria-hidden="true"><span class="octicon octicon-link"></span></a>Journal Papers</h2>
        
        <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://ggGaming-png.github.io/research/Fast_Manifold_Ranking_With_Local_Bipartite_Graph.pdf"> Fast Manifold Ranking With Local Bipartite Graph</a>  <br> <i>IEEE Transactions on Image Processing, 2021,30:6744-6756</i>  <br><button class="accordion"> 
        Abstract
        </button>   
        <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>During the past decades, manifold ranking has been widely applied to content-based image retrieval and shown excellent performance. However, manifold ranking is computationally expensive in both graph construction and ranking learning. Much effort has been devoted to improve its performance by introducing approximating techniques. In this paper, we propose a fast manifold ranking method, namely Local Bipartite Manifold Ranking (LBMR). Given a set of images, we first extract multiple regions from each image to form a large image descriptor matrix, and then use the anchor-based strategy to construct a local bipartite graph in which a regional k-means (RKM) is proposed to obtain high quality anchors. We propose an iterative method to directly solve the manifold ranking problem from the local bipartite graph, which monotonically decreases the objective function value in each iteration until the algorithm converges. Experimental results on several real-world image datasets demonstrate the effectiveness and efficiency of our proposed method. </p></div>
        <p style="margin:0"><button class="accordion">
          BibTeX citation
        </button>
        <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <pre> <code>
@article{chenFastManifoldRanking2021,
title = {Fast Manifold Ranking With Local Bipartite Graph},
author = {Chen, Xiaojun and Ye, Yuzhong and Wu, Qingyao and Nie, Feiping},
year = {2021},
journal = {IEEE Transactions on Image Processing},
volume = {30},
pages = {6744--6756},
issn = {1057-7149, 1941-0042},
doi = {10.1109/TIP.2021.3096082}
}
</code> </pre> </p></div>

        <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://ggGaming-png.github.io/research/Semisupervised_Feature_Selection_With_Sparse_Discriminative_Least_Squares_Regression.pdf">Semisupervised Feature Selection With Sparse Discriminative Least Squares Regression</a> <br> <i>IEEE Transactions on Cybernetics, 2021, PP(99): 1-12</i> <br><button class="accordion"> 
        Abstract   
        </button>   
        <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>In big data time, selecting informative features has become an urgent need. However, due to the huge cost of obtaining enough labeled data for supervised tasks, researchers have turned their attention to semisupervised learning, which exploits both labeled and unlabeled data. In this article, we propose a sparse discriminative semisupervised feature selection (SDSSFS) method. In this method, the €-dragging technique for the supervised task is extended to the semisupervised task, which is used to enlarge the distance between classes in order to obtain a discriminative solution. The flexible l_2,p norm is implicitly used as regularization in the new model. Therefore, we can obtain a more sparse solution by setting smaller p. An iterative method is proposed to simultaneously learn the regression coefficients and €-dragging matrix and predicting the unknown class labels. Experimental results on ten real-world datasets show the superiority of our proposed method  </p></div>
        <p style="margin:0"><button class="accordion">
          BibTeX citation
        </button>
        <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <pre> <code>
  @article{wangSemisupervisedFeatureSelection2022,
  title = {Semisupervised Feature Selection With Sparse Discriminative Least Squares Regression},
  author = {Wang, Chen and Chen, Xiaojun and Yuan, Guowen and Nie, Feiping and Yang, Min},
  year = {2022},
  month = aug,
  journal = {IEEE Transactions on Cybernetics},
  volume = {52},
  number = {8},
  pages = {8413--8424},
  issn = {2168-2267, 2168-2275},
  doi = {10.1109/TCYB.2021.3060804}
  }
    </code> </pre> </p></div>
        
        <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://ggGaming-png.github.io/research/Enhanced Balanced Min Cut.pdf"> Enhanced Balanced Min Cut </a> <br> <i>International Journal of Computer Vision, 2020,128(7): 1982-1995</i> <br><button class="accordion"> 
        Abstract   
        </button>   
        <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>Spectral clustering is a hot topic and many spectral clustering algorithms have been proposed. These algorithms usually solve the discrete cluster indicator matrix by relaxing the original problems, obtaining the continuous solution and finally obtaining a discrete solution that is close to the continuous solution. However, such methods often result in a non-optimal solution to the original problem since the different steps solve different problems. In this paper, we propose a novel spectral clustering method, named as Enhanced Balanced Min Cut (EBMC). In the new method, a new normalized cut model is proposed, in which a set of balance parameters are learned to capture the differences among different clusters. An iterative method with proved convergence is used to effectively solve the new model without eigendecomposition. Theoretical analysis reveals the connection between EBMC and the classical normalized cut. Extensive experimental results show the effectiveness and efficiency of our approach in comparison with the state-of-the-art methods.</p></div>
        <p style="margin:0"><button class="accordion">
          BibTeX citation
        </button>
        <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <pre> <code>
@article{chenEnhancedBalancedMin2020,
 title = {Enhanced Balanced Min Cut},
author = {Chen, Xiaojun and Hong, Weijun and Nie, Feiping and Huang, Joshua Zhexue and Shen, Li},
year = {2020},
month = jul,
journal = {International Journal of Computer Vision},
volume = {128},
number = {7},
pages = {1982--1995},
issn = {0920-5691, 1573-1405},
doi = {10.1007/s11263-020-01320-3}
}
    </code> </pre> </p></div>
    
        <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://ggGaming-png.github.io/research/Semi-Supervised_Feature_Selection_via_Sparse_Rescaled_Linear_Square_Regression.pdf"> Semi-Supervised Feature Selection via Sparse Rescaled Linear Square Regression </a><br> <i>IEEE Transactions on Knowledge & Data Engineering, 2020, 32(1): 165-176</i> <br><button class="accordion"> 
          Abstract   
          </button>   
          <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>Spectral clustering is a hot topic and many spectral clustering algorithms have been proposed. These algorithms usually solve the discrete cluster indicator matrix by relaxing the original problems, obtaining the continuous solution and finally obtaining a discrete solution that is close to the continuous solution. However, such methods often result in a non-optimal solution to the original problem since the different steps solve different problems. In this paper, we propose a novel spectral clustering method, named as Enhanced Balanced Min Cut (EBMC). In the new method, a new normalized cut model is proposed, in which a set of balance parameters are learned to capture the differences among different clusters. An iterative method with proved convergence is used to effectively solve the new model without eigendecomposition. Theoretical analysis reveals the connection between EBMC and the classical normalized cut. Extensive experimental results show the effectiveness and efficiency of our approach in comparison with the state-of-the-art methods.</p></div>
          <p style="margin:0"><button class="accordion">
            BibTeX citation
          </button>
          <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <pre> <code>
@article{chenSemiSupervisedFeatureSelection2020,
title = {Semi-Supervised Feature Selection via Sparse Rescaled Linear Square Regression},
author = {Chen, Xiaojun and Yuan, Guowen and Nie, Feiping and Ming, Zhong},
year = {2020},
month = jan,
journal = {IEEE Transactions on Knowledge and Data Engineering},
volume = {32},
number = {1},
pages = {165--176},
issn = {1041-4347, 1558-2191, 2326-3865},
doi = {10.1109/TKDE.2018.2879797}
}
      </code> </pre> </p></div>

        <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://ggGaming-png.github.io/research/Local_Adaptive_Projection_Framework_for_Feature_Selection_of_Labeled_and_Unlabeled_Data.pdf"> Local Adaptive Projection Framework for Feature Selection of Labeled and Unlabeled Data</a><br> <i>IEEE Transactions on Neural Networks & Learning Systems, 2019,49(9):3230-3241</i> <br><button class="accordion"> 
            Abstract   
            </button>   
            <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>Most feature selection methods first compute a similarity matrix by assigning a fixed value to pairs of objects in the whole data or to pairs of objects in a class or by computing the similarity between two objects from the original data. The similarity matrix is fixed as a constant in the subsequent feature selection process. However, the similarities computed from the original data may be unreliable, because they are affected by noise features. Moreover, the local structure within classes cannot be recovered if the similarities between the pairs of objects in a class are equal. In this paper, we propose a novel local adaptive projection (LAP) framework. Instead of computing fixed similarities before performing feature selection, LAP simultaneously learns an adaptive similarity matrix S and a projection matrix W with an iterative method. In each iteration, S is computed from the projected distance with the learned W and W is computed with the learned S. Therefore, LAP can learn better projection matrix W by weakening the effect of noise features with the adaptive similarity matrix. A supervised feature selection with LAP (SLAP) method and an unsupervised feature selection with LAP (ULAP) method are proposed. Experimental results on eight data sets show the superiority of SLAP compared with seven supervised feature selection methods and the superiority of ULAP compared with five unsupervised feature selection methods.</p></div>
        <p style="margin:0"><button class="accordion">
          BibTeX citation
        </button>
        <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <pre> <code>
@article{chenLocalAdaptiveProjection2018,
title = {Local Adaptive Projection Framework for Feature Selection of Labeled and Unlabeled Data},
author = {Chen, Xiaojun and Yuan, Guowen and Wang, Wenting and Nie, Feiping and Chang, Xiaojun and Huang, Joshua Zhexue},
year = {2018},
month = dec,
journal = {IEEE Transactions on Neural Networks and Learning Systems},
volume = {29},
number = {12},
pages = {6362--6373},
issn = {2162-237X, 2162-2388},
doi = {10.1109/TNNLS.2018.2830186}
}
    </code> </pre> </p></div>   

        <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://ggGaming-png.github.io/research\LABIN_Balanced_Min_Cut_for_Large-Scale_Data.pdf">LABIN: Balanced Min Cut for Large-Scale Data </a><br> <i>IEEE Transactions on Neural Networks and Learning Systems, 2019, 31(3): 725 - 736</i> <br><button class="accordion"> 
            Abstract   
            </button>   
          <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>Although many spectral clustering algorithms have been proposed during the past decades, they are not scalable to large-scale data due to their high computational complexities. In this paper, we propose a novel spectral clustering method for large-scale data, namely, large-scale balanced min cut (LABIN). A new model is proposed to extend the self-balanced min-cut (SBMC) model with the anchor-based strategy and a fast spectral rotation with linear time complexity is proposed to solve the new model. Extensive experimental results show the superior performance of our proposed method in comparison with the state-of-the-art methods including SBMC.</p></div>
          <p style="margin:0"><button class="accordion">
            BibTeX citation
          </button>
          <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <pre> <code>
@article{chenLABINBalancedMin2020,
title = {LABIN: Balanced Min Cut for Large-Scale Data},
shorttitle = {LABIN},
author = {Chen, Xiaojun and Chen, Renjie and Wu, Qingyao and Fang, Yixiang and Nie, Feiping and Huang, Joshua Zhexue},
year = {2020},
month = mar,
journal = {IEEE Transactions on Neural Networks and Learning Systems},
volume = {31},
number = {3},
pages = {725--736},
issn = {2162-237X, 2162-2388},
doi = {10.1109/TNNLS.2019.2909425}
}
      </code> </pre> </p></div> 
          
          <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://ggGaming-png.github.io/research/Subspace_Weighting_Co-Clustering_of_Gene_Expression_Data.pdf">Subspace Weighting Co-Clustering
            of Gene Expression Data </a><br> <i>IEEE/ACM Transactions on Computational Biology and Bioinformatics, 2019, 16(2): 352--364</i> <br><button class="accordion"> 
            Abstract   
            </button>   
          <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>Microarray technology enables the collection of vast amounts of gene expression data from biological experiments. Clustering algorithms have been successfully applied to exploring the gene expression data. Since a set of genes may be only correlated to a subset of samples, it is useful to use co-clustering to recover co-clusters in the gene expression data. In this paper, we propose a novel algorithm, called Subspace Weighting Co-Clustering (SWCC), for high dimensional gene expression data. In SWCC, a gene subspace weight matrix is introduced to identify the contribution of gene objects in distinguishing different sample clusters. We design a new co-clustering objective function to recover the co-clusters in the gene expression data, in which the subspace weight matrix is introduced. An iterative algorithm is developed to solve the objective function, in which the subspace weight matrix is automatically computed during the iterative co-clustering process. Our empirical study shows encouraging results of the proposed algorithm in comparison with six state-of-the-art clustering algorithms on ten gene expression data sets. We also propose to use SWCC for gene clustering and selection. The experimental results show that the selected genes can improve the classification performance of Random Forests.</p></div>        
          <p style="margin:0"><button class="accordion">
            BibTeX citation
          </button>
          <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <pre> <code>
@article{chenSubspaceWeightingCoClustering2019,
title = {Subspace Weighting Co-Clustering of Gene Expression Data},
author = {Chen, Xiaojun and Huang, Joshua Z. and Wu, Qingyao and Yang, Min},
year = {2019},
month = mar,
journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics},
volume = {16},
number = {2},
pages = {352--364},
issn = {1545-5963, 1557-9964, 2374-0043},
doi = {10.1109/TCBB.2017.2705686}
}
          
      </code> </pre> </p></div>

          <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://ggGaming-png.github.io/research/TWCC.pdf">TWCC: Automated Two-way Subspace Weighting Partitional
            Co-Clustering </a><br> <i>Pattern Recognition, 2018, 76: 404--415</i> <br><button class="accordion"> 
            Abstract   
            </button>   
          <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>A two-way subspace weighting partitional co-clustering method TWCC is proposed. In this method, two types of subspace weights are introduced to simultaneously weight the data in two ways, i.e., columns on row clusters and rows on column clusters. An objective function that uses the two types of weights in the distance function to determine the co-clusters of data is defined, and an iterative TWCC co-clustering algorithm to optimize the objective function is proposed, in which the two types of subspace weights are automatically computed. A series of experiments on both synthetic and real-life data were conducted to investigate the properties of TWCC, compare the two-way clustering results of TWCC with those of eight co-clustering algorithms, and compare one-way clustering results of TWCC with those of six clustering algorithms. The results have shown that TWCC is robust and effective for large high-dimensional data.</p></div> 
          <p style="margin:0"><button class="accordion">
            BibTeX citation
          </button>
          <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <pre> <code>
@article{chenTWCCAutomatedTwoway2018,
title = {TWCC: Automated Two-way Subspace Weighting Partitional Co-Clustering},
shorttitle = {TWCC},
author = {Chen, Xiaojun and Yang, Min and Zhexue Huang, Joshua and Ming, Zhong},
year = {2018},
month = apr,
journal = {Pattern Recognition},
volume = {76},
pages = {404--415},
issn = {00313203},
doi = {10.1016/j.patcog.2017.10.026}
}
      </code> </pre> </p></div>

          <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://ggGaming-png.github.io/research/PurTreeClust_A_Clustering_Algorithm_for_Customer_Segmentation_from_Massive_Customer_Transaction_Data.pdf">PurTreeClust: A Clustering Algorithm for
            Customer Segmentation from Massive
            Customer Transaction Data </a><br> <i>IEEE Transactions on Knowledge and Data Engineering, 30(3): 559-572 (2018)</i> <br><button class="accordion"> 
            Abstract   
            </button>   
          <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>Clustering of customer transaction data is an important procedure to analyze customer behaviors in retail and e-commerce companies. Note that products from companies are often organized as a product tree, in which the leaf nodes are goods to sell, and the internal nodes (except root node) could be multiple product categories. Based on this tree, we propose the “personalized product tree”, named purchase tree, to represent a customer’s transaction records. So the customers’ transaction data set can be compressed into a set of purchase trees. We propose a partitional clustering algorithm, named PurTreeClust, for fast clustering of purchase trees. A new distance metric is proposed to effectively compute the distance between two purchase trees. To cluster the purchase tree data, we first rank the purchase trees as candidate representative trees with a novel separate density, and then select the top k customers as the representatives of k customer groups. Finally, the clustering results are obtained by assigning each customer to the nearest representative. We also propose a gap statistic based method to evaluate the number of clusters. A series of experiments were conducted on ten real-life transaction data sets, and experimental results show the superior performance of the proposed method.</p></div>
          <p style="margin:0"><button class="accordion">
            BibTeX citation
          </button>
          <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <pre> <code>
@article{chenPurTreeClustClusteringAlgorithm2018,
title = {PurTreeClust: A Clustering Algorithm for Customer Segmentation from Massive Customer Transaction Data},
shorttitle = {PurTreeClust},
author = {Chen, Xiaojun and Fang, Yixiang and Yang, Min and Nie, Feiping and Zhao, Zhou and Huang, Joshua Zhexue},
year = {2018},
month = mar,
journal = {IEEE Transactions on Knowledge and Data Engineering},
volume = {30},
number = {3},
pages = {559--572},
issn = {1041-4347, 1558-2191, 2326-3865},
doi = {10.1109/TKDE.2017.2763620}
}
      </code> </pre> </p></div>

          <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://ggGaming-png.github.io/research/TW-k-means_Automated_two-level_variable_weighting_clustering_algorithm_for_multiview_data.pdf">TW-k-Means: Automated Two-Level Variable
            Weighting Clustering Algorithm for
            Multiview Data </a><br> <i> IEEE Transactions on Knowledge and Data Engineering, 2013, 25(4): 932--944</i> <br><button class="accordion"> 
            Abstract   
            </button>   
          <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>This paper proposes TW-k-means, an automated two-level variable weighting clustering algorithm for multiview data, which can simultaneously compute weights for views and individual variables. In this algorithm, a view weight is assigned to each view to identify the compactness of the view and a variable weight is also assigned to each variable in the view to identify the importance of the variable. Both view weights and variable weights are used in the distance function to determine the clusters of objects. In the new algorithm, two additional steps are added to the iterative k-means clustering process to automatically compute the view weights and the variable weights. We used two real-life data sets to investigate the properties of two types of weights in TW-k-means and investigated the difference between the weights of TW-k-means and the weights of the individual variable weighting method. The experiments have revealed the convergence property of the view weights in TW-k-means. We compared TW-k-means with five clustering algorithms on three real-life data sets and the results have shown that the TW-k-means algorithm significantly outperformed the other five clustering algorithms in four evaluation indices.</p></div>
          <p style="margin:0"><button class="accordion">
            BibTeX citation
          </button>
          <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <pre> <code>
@article{xiaojunchenTWkmeansAutomatedTwolevel2013,
title = {TW-k-means: Automated Two-Level Variable Weighting Clustering Algorithm for Multiview Data},
shorttitle = {TW-k-means},
author = {Xiaojun Chen and Xiaofei Xu and Huang, J. Z. and Yunming Ye},
year = {2013},
month = apr,
journal = {IEEE Transactions on Knowledge and Data Engineering},
volume = {25},
number = {4},
pages = {932--944},
issn = {1041-4347},
doi = {10.1109/TKDE.2011.262}
}
            
      </code> </pre> </p></div>

          <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://ggGaming-png.github.io/research/A Feature group weighting.pdf">A feature group weighting method for subspace clustering
            of high-dimensional data </a><br> <i>Pattern Recognition, 2012, 45(1): 434--446</i> <br><button class="accordion"> 
            Abstract   
            </button>   
          <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>This paper proposes a new method to weight subspaces in feature groups and individual features for clustering high-dimensional data. In this method, the features of high-dimensional data are divided into feature groups, based on their natural characteristics. Two types of weights are introduced to the clustering process to simultaneously identify the importance of feature groups and individual features in each cluster. A new optimization model is given to define the optimization process and a new clustering algorithm FG-k-means is proposed to optimize the optimization model. The new algorithm is an extension to k-means by adding two additional steps to automatically calculate the two types of subspace weights. A new data generation method is presented to generate high-dimensional data with clusters in subspaces of both feature groups and individual features. Experimental results on synthetic and real-life data have shown that the FG-k-means algorithm significantly outperformed four k-means type algorithms, i.e., k-means, W-k-means, LAC and EWKM in almost all experiments. The new algorithm is robust to noise and missing values which commonly exist in high-dimensional data.</p></div>
          <p style="margin:0"><button class="accordion">
            BibTeX citation
          </button>
          <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <pre> <code>
@article{chenFeatureGroupWeighting2012,
title = {A Feature Group Weighting Method for Subspace Clustering of High-Dimensional Data},
author = {Chen, Xiaojun and Ye, Yunming and Xu, Xiaofei and Huang, Joshua Zhexue},
year = {2012},
month = jan,
journal = {Pattern Recognition},
volume = {45},
number = {1},
pages = {434--446},
issn = {00313203},
doi = {10.1016/j.patcog.2011.06.004}
}
      </code> </pre> </p></div>
          <br><br>
          <hr>
          <h2><a id="published-papers-updated" class="anchor" href="#publications" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conference Papers</h2>

          <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://ggGaming-png.github.io/research/Deep Self-Adaptive.pdf"> Deep Self-Adaptive Hashing for Image Retrieval</a><br> <i>CIKM '21: The 30th ACM International Conference on Information and Knowledge Management, Virtual Event, Queensland, Australia, November 1 - 5, 2021, ACM, 2021: 1028--1037</i> <br><button class="accordion"> 
            Abstract   
            </button>   
          <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>Hashing technology has been widely used in image retrieval due to its computational and storage efficiency. Recently, deep unsupervised hashing methods have attracted increasing attention due to the high cost of human annotations in the real world and the superiority of deep learning technology. However, most deep unsupervised hashing methods usually pre-compute a similarity matrix to model the pairwise relationship in the pre-trained feature space. Then this similarity matrix would be used to guide hash learning, in which most of the data pairs are treated equivalently. The above process is confronted with the following defects: 1) The pre-computed similarity matrix is inalterable and disconnected from the hash learning process, which cannot explore the underlying semantic information. 2) The informative data pairs may be buried by the large number of less-informative data pairs. To solve the aforementioned problems, we propose a Deep Self-Adaptive Hashing (DSAH) model to adaptively capture the semantic information with two special designs: Adaptive Neighbor Discovery (AND) and Pairwise Information Content (PIC). Firstly, we adopt the AND to initially construct a neighborhood-based similarity matrix, and then refine this initial similarity matrix with a novel update strategy to further investigate the semantic structure behind the learned representation. Secondly, we measure the priorities of data pairs with PIC and assign adaptive weights to them, which is relies on the assumption that more dissimilar data pairs contain more discriminative information for hash learning. Extensive experiments on several datasets demonstrate that the above two technologies facilitate the deep hashing model to achieve superior performance.</p></div>
          <p style="margin:0"><button class="accordion">
            BibTeX citation
          </button>
          <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <pre> <code>
@inproceedings{linDeepSelfAdaptiveHashing2021,
title = {Deep Self-Adaptive Hashing for Image Retrieval},
booktitle = {Proceedings of the 30th ACM International Conference on Information \& Knowledge Management},
author = {Lin, Qinghong and Chen, Xiaojun and Zhang, Qin and Tian, Shangxuan and Chen, Yudong},
year = {2021},
month = oct,
pages = {1028--1037},
publisher = {ACM},
address = {Virtual Event Queensland Australia},
doi = {10.1145/3459637.3482247},
isbn = {978-1-4503-8446-9}
}
      </code> </pre> </p></div>

          <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://ggGaming-png.github.io/research/Spectral Clusterinf.pdf"> Spectral Clustering of Large-scale Data by Directly Solving
            Normalized Cut</a><br> <i> Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2018: 1206--1215</i> <br><button class="accordion"> 
            Abstract   
            </button>   
          <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>During the past decades, many spectral clustering algorithms have been proposed. However, their high computational complexities hinder their applications on large-scale data. Moreover, most of them use a two-step approach to obtain the optimal solution, which may deviate from the solution by directly solving the original problem. In this paper, we propose a new optimization algorithm, namely Direct Normalized Cut (DNC), to directly optimize the normalized cut model. DNC has a quadratic time complexity, which is a significant reduction comparing with the cubic time complexity of the traditional spectral clustering. To cope with large-scale data, a Fast Normalized Cut (FNC) method with linear time and space complexities is proposed by extending DNC with an anchor-based strategy. In the new method, we first seek a set of anchors and then construct a representative similarity matrix by computing distances between the anchors and the whole data set. To find high quality anchors that best represent the whole data set, we propose a Balanced k-means (BKM) to partition a data set into balanced clusters and use the cluster centers as anchors. Then DNC is used to obtain the final clustering result from the representative similarity matrix. A series of experiments were conducted on both synthetic data and real-world data sets, and the experimental results show the superior performance of BKM, DNC and FNC.</p></div>
<p style="margin:0"><button class="accordion">
          BibTeX citation
        </button>
        <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <pre> <code>
@inproceedings{chenSpectralClusteringLargescale2018,
title = {Spectral Clustering of Large-scale Data by Directly Solving Normalized Cut},
booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
author = {Chen, Xiaojun and Hong, Weijun and Nie, Feiping and He, Dan and Yang, Min and Huang, Joshua Zhexue},
year = {2018},
month = jul,
pages = {1206--1215},
publisher = {ACM},
address = {London United Kingdom},
doi = {10.1145/3219819.3220039},
isbn = {978-1-4503-5552-0}
}
    </code> </pre> </p></div>

          <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://ggGaming-png.github.io/research/A_Self-Balanced_Min-Cut_Algorithm_for_Image_Clustering (1).pdf">A Self-Balanced Min-Cut Algorithm for Image Clustering </a><br> <i>International Conference on Computer Vision, 2017: 2080--2088</i> <br><button class="accordion"> 
            Abstract   
            </button>   
          <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>Many spectral clustering algorithms have been proposed and successfully applied to image data analysis such as content based image retrieval, image annotation, and image indexing. Conventional spectral clustering algorithms usually involve a two-stage process: eigendecomposition of similarity matrix and clustering assignments from eigenvectors by k-means or spectral rotation. However, the final clustering assignments obtained by the two-stage process may deviate from the assignments by directly optimize the original objective function. Moreover, most of these methods usually have very high computational complexities. In this paper, we propose a new min-cut algorithm for image clustering, which scales linearly to the data size. In the new method, a self-balanced min-cut model is proposed in which the Exclusive Lasso is implicitly introduced as a balance regularizer in order to produce balanced partition. We propose an iterative algorithm to solve the new model, which has a time complexity of O(n) where n is the number of samples. Theoretical analysis reveals that the new method can simultaneously minimize the graph cut and balance the partition across all clusters. A series of experiments were conducted on both synthetic and benchmark data sets and the experimental results show the superior performance of the new method.</p></div>
<p style="margin:0"><button class="accordion">
          BibTeX citation
        </button>
        <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <pre> <code>
@inproceedings{chenSelfBalancedMinCutAlgorithm2017,
title = {A Self-Balanced Min-Cut Algorithm for Image Clustering},
booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
author = {Chen, Xiaojun and Haung, Joshua Zhexue and Nie, Feiping and Chen, Renjie and Wu, Qingyao},
year = {2017},
month = oct,
pages = {2080--2088},
publisher = {IEEE},
address = {Venice},
doi = {10.1109/ICCV.2017.227},
isbn = {978-1-5386-1032-9}
}
    </code> </pre> </p></div>

          <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://ggGaming-png.github.io/research/Semi-supervised Feature Selection via Rescaled Linear Regression.pdf">Semi-supervised Feature Selection via Rescaled Linear Regression </a><br> <i>Twenty-Sixth International Joint Conference on Artificial Intelligence, 2017: 1525-1531</i> <br><button class="accordion"> 
            Abstract   
            </button>   
          <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>With the rapid increase of complex and high-dimensional sparse data, demands for new methods to select features by exploiting both labeled and unlabeled data have increased. Least regression based feature selection methods usually learn a projection matrix and evaluate the importances of features using the projection matrix, which is lack of theoretical explanation. Moreover, these methods cannot find both global and sparse solution of the projection matrix. In this paper, we propose a novel semi-supervised feature selection method which can learn both global and sparse solution of the projection matrix. The new method extends the least square regression model by rescaling the regression coefficients in the least square regression with a set of scale factors, which are used for ranking the features. It has shown that the new model can learn global and sparse solution. Moreover, the introduction of scale factors provides a theoretical explanation for why we can use the projection matrix to rank the features. A simple yet effective algorithm with proved convergence is proposed to optimize the new model. Experimental results on eight real-life data sets show the superiority of the method.</p></div>
<p style="margin:0"><button class="accordion">
          BibTeX citation
        </button>
        <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <pre> <code>
@inproceedings{chenSemisupervisedFeatureSelection2017,
title = {Semi-Supervised Feature Selection via Rescaled Linear Regression},
booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence},
author = {Chen, Xiaojun and Yuan, Guowen and Nie, Feiping and Huang, Joshua Zhexue},
year = {2017},
month = aug,
pages = {1525--1531},
publisher = {International Joint Conferences on Artificial Intelligence Organization},
address = {Melbourne, Australia},
doi = {10.24963/ijcai.2017/211},
isbn = {978-0-9992411-0-3}
}
    </code> </pre> </p></div>

          <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://ggGaming-png.github.io/research/Scalable Normalized.pdf">Scalable Normalized Cut with Improved Spectral Rotation</a><br> <i> Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, 2017: 1518--1524</i> <br><button class="accordion"> 
            Abstract   
            </button>   
          <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>Many spectral clustering algorithms have been proposed and successfully applied to many high-dimensional applications. However, there are still two problems that need to be solved: 1) existing methods for obtaining the final clustering assignments may deviate from the true discrete solution, and 2) most of these methods usually have very high computational complexity. In this paper, we propose a Scalable Normalized Cut method for clustering of large scale data. In the new method, an efficient method is used to construct a small representation matrix and then clustering is performed on the representation matrix. In the clustering process, an improved spectral rotation method is proposed to obtain the solution of the final clustering assignments. A series of experimental were conducted on 14 benchmark data sets and the experimental results show the superior performance of the new method.</p></div>
<p style="margin:0"><button class="accordion">
          BibTeX citation
        </button>
        <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <pre> <code>
@inproceedings{chenScalableNormalizedCut2017,
title = {Scalable Normalized Cut with Improved Spectral Rotation},
booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence},
author = {Chen, Xiaojun and Nie, Feiping and Huang, Joshua Zhexue and Yang, Min},
year = {2017},
month = aug,
pages = {1518--1524},
publisher = {International Joint Conferences on Artificial Intelligence Organization},
address = {Melbourne, Australia},
doi = {10.24963/ijcai.2017/210},
isbn = {978-0-9992411-0-3}
}
    </code> </pre> </p></div>

          <p style="margin:0"> <a style="margin:0; font-size:100%; font-weight:bold" href="https://ggGaming-png.github.io/research/PurTreeClust_A_purchase_tree_clustering_algorithm_for_large-scale_customer_transaction_data.pdf">PurTreeClust: A Purchase Tree Clustering Algorithm 
            for Large-scale Customer Transaction Data  </a><br> <i>International Conference on Data Engineering</i> <br><button class="accordion"> 
            Abstract   
            </button>   
          <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p>Clustering of customer transaction data is usually an important procedure to analyze customer behaviors in retail and e-commerce companies. Note that products from companies are often organized as a product tree, in which the leaf nodes are goods to sell, and the internal nodes (except root node) could be multiple product categories. Based on this tree, we present to use a "personalized product tree", called purchase tree, to represent a customer's transaction data. The customer transaction data set can be represented as a set of purchase trees. We propose a PurTreeClust algorithm for clustering of large-scale customers from purchase trees. We define a new distance metric to effectively compute the distance between two purchase trees from the entire levels in the tree. A cover tree is then built for indexing the purchase tree data and we propose a leveled density estimation method for selecting initial cluster centers from a cover tree. PurTreeClust, a fast clustering method for clustering of large-scale purchase trees, is then presented. Last, we propose a gap statistic based method for estimating the number of clusters from the purchase tree clustering results. A series of experiments were conducted on ten large-scale transaction data sets which contain up to four million transaction records, and experimental results have verified the effectiveness and efficiency of the proposed method. We also compared our method with three clustering algorithms, e.g., spectral clustering, hierarchical agglomerative clustering and DBSCAN. The experimental results have demonstrated the superior performance of the proposed method.</p></div>
<p style="margin:0"><button class="accordion">
          BibTeX citation
        </button>
        <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"><p> <pre> <code>
@inproceedings{chenPurTreeClustPurchaseTree2016,
title = {PurTreeClust: A Purchase Tree Clustering Algorithm for Large-Scale Customer Transaction Data},
shorttitle = {PurTreeClust},
booktitle = {2016 IEEE 32nd International Conference on Data Engineering (ICDE)},
author = {Chen, Xiaojun and Huang, Joshua Zhexue and Luo, Jun},
year = {2016},
month = may,
pages = {661--672},
publisher = {IEEE},
address = {Helsinki, Finland},
doi = {10.1109/ICDE.2016.7498279},
isbn = {978-1-5090-2020-1}
}
    </code> </pre> </p></div>

      </section>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    <script> 
    var acc = document.getElementsByClassName("accordion");
    var i;

    for (i = 0; i < acc.length; i++) {
        acc[i].onclick = function(){
            this.classList.toggle("active");
            this.parentNode.nextElementSibling.classList.toggle("show");
      }
    }
    </script>
  </body>
</html>
